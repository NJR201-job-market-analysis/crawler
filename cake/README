# Cake 爬蟲系統

程式碼位於 https://github.com/NJR201-job-market-analysis/crawler/cake

一個基於 Celery 和 RabbitMQ 的分布式爬蟲系統，用於爬取 Cake 平台的職缺資訊。

## 📁 目錄結構

```
/root/job-crawler/cake/
├── setup.py                    # Python 包配置
├── Pipfile                     # Python 依賴管理
├── local.ini                   # 環境配置文件
├── genenv.py                   # 環境變數生成器
├── .env                        # 環境變數文件（自動生成）
├── crawler/                    # 爬蟲模組目錄
│   ├── __init__.py
│   ├── cake_config.py          # 配置管理
│   ├── cake_worker.py          # Celery Worker 配置
│   ├── cake_tasks.py           # Celery 任務定義
│   ├── cake_producer.py        # 任務生產者
│   ├── cake_crawler.py         # 核心爬蟲邏輯
│   └── main.py                 # 主程序入口
└── docker-compose.monitor.yml  # Docker 監控配置
```

## 🚀 快速開始

### 1. 環境準備

```bash
# 進入項目目錄
cd /root/job-crawler/cake

# 生成環境變數文件
ENV=DOCKER|DEV python genenv.py

# 驗證配置
pipenv run python -c "from crawler.cake_config import RABBITMQ_HOST; print(f'RABBITMQ_HOST: {RABBITMQ_HOST}')"

# 安裝依賴
pipenv install
```

### 2. 啟動 RabbitMQ、MySQL、Flower、PhpMyAdmin

```bash
# 使用 Docker 啟動
docker compose -f docker-compose.monitor.yml up -d
docker compose -f docker-compose.monitor.yml down -d

# 查看容器狀態
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

### 3: 啟動 Worker、Producer

```bash
# 終端 1: 啟動 Worker
pipenv run celery -A crawler.cake_worker worker --loglevel=info

# 終端 3: 執行 Producer
pipenv run python -m crawler.cake_producer
```

### 監控任務執行

- **Worker 終端**: 查看任務執行日誌
- **Flower 界面**: 查看任務狀態和統計 (http://localhost:5555)
- **RabbitMQ 管理界面**: 查看隊列狀態 (http://localhost:15672)

### 任務配置

```python
# cake_producer.py
crawl_cake_jobs.delay(
    category="software-developer",    # 職缺類別
    job_type="it_front-end-engineer", # 職位類型
    filename="cake_jobs.csv",         # 輸出文件名
)
```

## 📊 程式碼架構

### 配置層 (Configuration Layer)
- **`cake_config.py`**: 環境變數管理，連接 RabbitMQ 和 MySQL
- **`local.ini`**: 多環境配置（DEV/DOCKER/PRODUCTION）
- **`genenv.py`**: 根據環境生成 `.env` 文件

### 任務層 (Task Layer)
- **`cake_worker.py`**: Celery 應用配置，連接 RabbitMQ
- **`cake_tasks.py`**: 定義爬蟲任務和測試任務
- **`cake_producer.py`**: 任務生產者，發送爬蟲任務

### 爬蟲層 (Crawler Layer)
- **`cake_crawler.py`**: 核心爬蟲邏輯，解析 Cake 網站

## 🔍 故障排除

### 常見問題

1. **RabbitMQ 連接失敗**
   ```bash
   # 檢查 RabbitMQ 是否運行
   docker ps | grep rabbitmq
   
   # 檢查端口是否開放
   netstat -tlnp | grep 5672
   ```

2. **Worker 無法啟動**
   ```bash
   # 檢查依賴是否安裝
   pipenv install
   
   # 檢查配置是否正確
   pipenv run python -c "from crawler.cake_config import *; print(RABBITMQ_HOST)"
   ```

3. **任務執行失敗**
   ```bash
   # 檢查 Worker 日誌
   pipenv run celery -A crawler.cake_worker worker --loglevel=debug
   
   # 檢查 Flower 監控界面
   # 訪問 http://localhost:5555
   ```

## 📄 輸出格式

爬蟲會生成 CSV 文件，包含以下欄位：
- 職缺
- 公司
- 要求技能
- 公司簡述
- 地點
- 薪資
- 經驗

## 🚀 如何測試 celery 是否正常運行

1. **進入虛擬環境**
   ```bash
   pipenv shell
   ```

2. **啟動 worker**
   ```bash
   # 如果已經在虛擬環境 (在本地開發要這樣使用)
   celery -A crawler.cake_worker worker --loglevel=info

   # 如果不在虛擬環境 (在 docker 中要這樣使用)
   pipenv run celery -A crawler.cake_worker worker --loglevel=info
   ```

3. **測試發送任務**
   ```bash
   pipenv run python -m crawler.cake_producer
   ```


資料表規劃

company
locations
job_type
job_posts
job_skills
